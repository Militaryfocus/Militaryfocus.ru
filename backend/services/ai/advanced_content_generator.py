"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
–í–∫–ª—é—á–∞–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ò–ò-–º–æ–¥–µ–ª–∏, –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é, SEO-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
"""

import os
import json
import time
import logging
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, asdict
from enum import Enum
import re
import random
from collections import defaultdict, Counter
import hashlib
import requests
from urllib.parse import urljoin

import openai
import anthropic
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from textstat import flesch_reading_ease, automated_readability_index

from models import Post, Category, Tag, Comment, User
from config.database import db
# –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥—É–ª–µ–π
class ValidationResult:
    def __init__(self, is_valid=True, score=0.8, issues=None):
        self.is_valid = is_valid
        self.score = score
        self.issues = issues or []

def ai_content_validator(content, title):
    return ValidationResult(is_valid=True, score=0.85)

class EnhancedAIContentGenerator:
    def generate(self, *args, **kwargs):
        return {"content": "Generated content", "title": "Generated title"}

def track_ai_content_generation(content_type, status, metadata=None):
    pass
from services.monitoring import monitoring_system

logger = logging.getLogger(__name__)

class ContentType(Enum):
    """–¢–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    HOW_TO_GUIDE = "how_to_guide"
    COMPARISON_REVIEW = "comparison_review"
    ANALYTICAL_ARTICLE = "analytical_article"
    NEWS_ARTICLE = "news_article"
    EXPERT_INTERVIEW = "expert_interview"
    CASE_STUDY = "case_study"
    LISTICLE = "listicle"
    TUTORIAL = "tutorial"
    OPINION_PIECE = "opinion_piece"
    RESEARCH_SUMMARY = "research_summary"

class ContentTone(Enum):
    """–¢–æ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    PROFESSIONAL = "professional"
    CONVERSATIONAL = "conversational"
    AUTHORITATIVE = "authoritative"
    FRIENDLY = "friendly"
    TECHNICAL = "technical"
    INSPIRATIONAL = "inspirational"
    CRITICAL = "critical"
    HUMOROUS = "humorous"

class TargetAudience(Enum):
    """–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è"""
    BEGINNERS = "beginners"
    INTERMEDIATE = "intermediate"
    EXPERTS = "experts"
    GENERAL_PUBLIC = "general_public"
    PROFESSIONALS = "professionals"
    STUDENTS = "students"
    ENTREPRENEURS = "entrepreneurs"

@dataclass
class ContentRequest:
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    topic: str
    content_type: ContentType
    tone: ContentTone
    target_audience: TargetAudience
    length: str  # "short", "medium", "long"
    keywords: List[str]
    exclude_keywords: List[str]
    include_images: bool = True
    include_quotes: bool = True
    include_statistics: bool = True
    include_examples: bool = True
    seo_optimized: bool = True
    personalized: bool = False
    user_preferences: Dict[str, Any] = None

@dataclass
class GeneratedContent:
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç"""
    title: str
    content: str
    excerpt: str
    meta_description: str
    tags: List[str]
    category: str
    content_type: ContentType
    tone: ContentTone
    target_audience: TargetAudience
    word_count: int
    reading_time: int
    seo_score: float
    readability_score: float
    engagement_score: float
    quality_score: float
    images_suggestions: List[Dict[str, str]]
    internal_links: List[Dict[str, str]]
    external_links: List[Dict[str, str]]
    call_to_action: str
    social_media_posts: Dict[str, str]
    generated_at: datetime
    processing_time: float

class ModernAIIntegration:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ò–ò-–º–æ–¥–µ–ª—è–º–∏"""
    
    def __init__(self):
        self.openai_client = None
        self.anthropic_client = None
        self.local_models = {}
        self.model_cache = {}
        self.init_ai_services()
    
    def init_ai_services(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ò–ò-—Å–µ—Ä–≤–∏—Å–æ–≤"""
        try:
            # OpenAI GPT-4
            openai_key = os.environ.get('OPENAI_API_KEY')
            if openai_key:
                self.openai_client = openai.OpenAI(api_key=openai_key)
                logger.info("‚úÖ OpenAI GPT-4 –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            # Anthropic Claude
            anthropic_key = os.environ.get('ANTHROPIC_API_KEY')
            if anthropic_key:
                self.anthropic_client = anthropic.Anthropic(api_key=anthropic_key)
                logger.info("‚úÖ Anthropic Claude –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            # –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
            self._load_local_models()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ò–ò-—Å–µ—Ä–≤–∏—Å–æ–≤: {e}")
    
    def _load_local_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            # –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            self.local_models['title_generator'] = pipeline(
                "text-generation",
                model="microsoft/DialoGPT-medium",
                tokenizer="microsoft/DialoGPT-medium",
                device=0 if torch.cuda.is_available() else -1
            )
            
            # –ú–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            self.local_models['sentiment_analyzer'] = pipeline(
                "sentiment-analysis",
                model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                device=0 if torch.cuda.is_available() else -1
            )
            
            logger.info("‚úÖ –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏: {e}")
    
    async def generate_with_gpt4(self, prompt: str, max_tokens: int = 2000, 
                               temperature: float = 0.7) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é GPT-4"""
        if not self.openai_client:
            raise Exception("OpenAI –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                frequency_penalty=0.1,
                presence_penalty=0.1
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å GPT-4: {e}")
            raise
    
    async def generate_with_claude(self, prompt: str, max_tokens: int = 2000) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é Claude"""
        if not self.anthropic_client:
            raise Exception("Anthropic –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        try:
            response = self.anthropic_client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=max_tokens,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            return response.content[0].text.strip()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å Claude: {e}")
            raise
    
    def generate_title_variations(self, topic: str, content_type: ContentType) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
        templates = {
            ContentType.HOW_TO_GUIDE: [
                "–ö–∞–∫ {topic}: –ø–æ—à–∞–≥–æ–≤–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ",
                "–ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ {topic}",
                "–ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ {topic}: —Å–æ–≤–µ—Ç—ã —ç–∫—Å–ø–µ—Ä—Ç–æ–≤",
                "–ò–∑—É—á–∞–µ–º {topic}: –æ—Ç –ê –¥–æ –Ø"
            ],
            ContentType.COMPARISON_REVIEW: [
                "{topic}: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ",
                "–û–±–∑–æ—Ä {topic}: —á—Ç–æ –≤—ã–±—Ä–∞—Ç—å?",
                "{topic}: –ø–æ–¥—Ä–æ–±–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤",
                "–õ—É—á—à–∏–µ {topic}: —Ä–µ–π—Ç–∏–Ω–≥ –∏ –∞–Ω–∞–ª–∏–∑"
            ],
            ContentType.ANALYTICAL_ARTICLE: [
                "–ê–Ω–∞–ª–∏–∑ {topic}: —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑—ã",
                "{topic}: –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å–∏—Ç—É–∞—Ü–∏–∏",
                "–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å {topic}: –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö",
                "{topic}: –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏ –≤—ã–≤–æ–¥—ã"
            ],
            ContentType.NEWS_ARTICLE: [
                "–ù–æ–≤–æ—Å—Ç–∏ {topic}: –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è",
                "{topic}: —á—Ç–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å?",
                "–ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ {topic}",
                "{topic}: —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ñ–∞–∫—Ç—ã"
            ],
            ContentType.LISTICLE: [
                "10 —Ñ–∞–∫—Ç–æ–≤ –æ {topic}, –∫–æ—Ç–æ—Ä—ã–µ –≤–∞—Å —É–¥–∏–≤—è—Ç",
                "–¢–æ–ø-5 {topic} –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö",
                "7 —Å–ø–æ—Å–æ–±–æ–≤ {topic}: –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã",
                "15 –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –æ {topic}"
            ]
        }
        
        title_templates = templates.get(content_type, [
            "–í—Å–µ –æ {topic}",
            "{topic}: –ø–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ",
            "–ò–∑—É—á–∞–µ–º {topic}",
            "{topic}: —á—Ç–æ –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å"
        ])
        
        titles = []
        for template in title_templates:
            title = template.format(topic=topic)
            titles.append(title)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏
            variations = [
                f"{title}: –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
                f"{title} –≤ 2024 –≥–æ–¥—É",
                f"–≠–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –º–Ω–µ–Ω–∏–µ: {title.lower()}",
                f"{title} - –ø–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ"
            ]
            titles.extend(variations)
        
        return titles[:10]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-10

class ContentPersonalization:
    """–°–∏—Å—Ç–µ–º–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self):
        self.user_profiles = {}
        self.content_preferences = defaultdict(list)
        self.reading_history = defaultdict(list)
        self.engagement_metrics = defaultdict(dict)
    
    def analyze_user_preferences(self, user_id: int) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_posts = Post.query.join(Comment).filter(Comment.author_id == user_id).all()
        
        preferences = {
            'preferred_categories': Counter(),
            'preferred_topics': Counter(),
            'preferred_length': 'medium',
            'preferred_tone': ContentTone.CONVERSATIONAL,
            'preferred_content_types': Counter(),
            'reading_time_preference': 5,  # –º–∏–Ω—É—Ç
            'engagement_level': 'medium'
        }
        
        for post in user_posts:
            if post.category:
                preferences['preferred_categories'][post.category.name] += 1
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–≥–∏
            for tag in post.tags:
                preferences['preferred_topics'][tag.name] += 1
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            word_count = len(post.content.split())
            if word_count < 500:
                preferences['preferred_length'] = 'short'
            elif word_count > 2000:
                preferences['preferred_length'] = 'long'
        
        return preferences
    
    def personalize_content_request(self, request: ContentRequest, 
                                  user_preferences: Dict[str, Any]) -> ContentRequest:
        """–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç"""
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º —Ç–æ–Ω –ø–æ–¥ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if user_preferences.get('preferred_tone'):
            request.tone = user_preferences['preferred_tone']
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –¥–ª–∏–Ω—É
        if user_preferences.get('preferred_length'):
            request.length = user_preferences['preferred_length']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–µ —Ç–µ–º—ã
        preferred_topics = user_preferences.get('preferred_topics', {})
        if preferred_topics:
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ø-3 –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–µ —Ç–µ–º—ã –∫ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            top_topics = [topic for topic, count in preferred_topics.most_common(3)]
            request.keywords.extend(top_topics)
        
        return request
    
    def adapt_content_for_user(self, content: str, user_preferences: Dict[str, Any]) -> str:
        """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        adapted_content = content
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å —è–∑—ã–∫–∞
        if user_preferences.get('engagement_level') == 'beginner':
            adapted_content = self._simplify_language(adapted_content)
        elif user_preferences.get('engagement_level') == 'expert':
            adapted_content = self._add_technical_details(adapted_content)
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –¥–ª–∏–Ω—É
        preferred_length = user_preferences.get('preferred_length', 'medium')
        if preferred_length == 'short':
            adapted_content = self._shorten_content(adapted_content)
        elif preferred_length == 'long':
            adapted_content = self._expand_content(adapted_content)
        
        return adapted_content
    
    def _simplify_language(self, content: str) -> str:
        """–£–ø—Ä–æ—â–µ–Ω–∏–µ —è–∑—ã–∫–∞"""
        # –ó–∞–º–µ–Ω—è–µ–º —Å–ª–æ–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ
        simplifications = {
            '–æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å': '–¥–µ–ª–∞—Ç—å',
            '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å': '—Å–æ–∑–¥–∞–≤–∞—Ç—å',
            '—è–≤–ª—è–µ—Ç—Å—è': '—ç—Ç–æ',
            '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å —Å–æ–±–æ–π': '–±—ã—Ç—å',
            '–∏–º–µ—Ç—å –º–µ—Å—Ç–æ': '–ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å',
            '–Ω–æ—Å–∏—Ç—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä': '–±—ã—Ç—å',
            '–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ': '–≤–ª–∏—è—Ç—å'
        }
        
        for complex_word, simple_word in simplifications.items():
            content = content.replace(complex_word, simple_word)
        
        return content
    
    def _add_technical_details(self, content: str) -> str:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π"""
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–∞–∑–¥–µ–ª—ã
        technical_sections = [
            "\n\n## –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏\n\n–î–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –≤–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –∞—Å–ø–µ–∫—Ç—ã:",
            "\n\n## –£–≥–ª—É–±–ª–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n\n–° —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –º–æ–∂–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å:",
            "\n\n## –≠–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –º–Ω–µ–Ω–∏–µ\n\n–û–ø—ã—Ç–Ω—ã–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã –æ—Ç–º–µ—á–∞—é—Ç:"
        ]
        
        if random.random() < 0.3:  # 30% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
            content += random.choice(technical_sections)
        
        return content
    
    def _shorten_content(self, content: str) -> str:
        """–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
        sections = content.split('\n\n')
        if len(sections) > 4:
            # –û—Å—Ç–∞–≤–ª—è–µ–º –≤–≤–µ–¥–µ–Ω–∏–µ, 2 –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–∞ –∏ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ
            shortened = sections[:1] + sections[1:3] + sections[-1:]
            return '\n\n'.join(shortened)
        
        return content
    
    def _expand_content(self, content: str) -> str:
        """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
        additional_sections = [
            "\n\n## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n\n–î–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–º—ã —Å—Ç–æ–∏—Ç —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å:",
            "\n\n## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã\n\n–í –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –≤–∞–∂–Ω–æ —É—á–∏—Ç—ã–≤–∞—Ç—å:",
            "\n\n## –ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã\n\n–ú–Ω–æ–≥–∏–µ —á–∏—Ç–∞—Ç–µ–ª–∏ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç—Å—è:"
        ]
        
        content += random.choice(additional_sections)
        return content

class SEOOptimizer:
    """SEO-–æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self):
        self.keyword_density_threshold = 0.02  # 2%
        self.max_keyword_density = 0.05  # 5%
        self.optimal_title_length = 60
        self.optimal_description_length = 160
        self.optimal_heading_structure = ['H1', 'H2', 'H3']
    
    def optimize_for_seo(self, content: str, title: str, keywords: List[str]) -> Dict[str, Any]:
        """SEO-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        seo_analysis = {
            'title_score': self._analyze_title_seo(title, keywords),
            'content_score': self._analyze_content_seo(content, keywords),
            'structure_score': self._analyze_structure_seo(content),
            'keyword_density': self._calculate_keyword_density(content, keywords),
            'readability_score': flesch_reading_ease(content),
            'meta_score': self._analyze_meta_elements(content, title),
            'internal_linking_score': self._analyze_internal_links(content),
            'overall_seo_score': 0.0
        }
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–π SEO-–±–∞–ª–ª
        seo_analysis['overall_seo_score'] = (
            seo_analysis['title_score'] * 0.2 +
            seo_analysis['content_score'] * 0.3 +
            seo_analysis['structure_score'] * 0.2 +
            seo_analysis['meta_score'] * 0.15 +
            seo_analysis['internal_linking_score'] * 0.15
        )
        
        return seo_analysis
    
    def _analyze_title_seo(self, title: str, keywords: List[str]) -> float:
        """–ê–Ω–∞–ª–∏–∑ SEO –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        score = 0.0
        
        # –î–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        if 30 <= len(title) <= self.optimal_title_length:
            score += 0.3
        elif len(title) > self.optimal_title_length:
            score += 0.1
        
        # –ù–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        title_lower = title.lower()
        for keyword in keywords:
            if keyword.lower() in title_lower:
                score += 0.3
                break
        
        # –ü–æ–∑–∏—Ü–∏—è –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
        if keywords and keywords[0].lower() in title_lower[:20]:
            score += 0.2
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        emotional_words = ['–ª—É—á—à–∏–π', '—Ç–æ–ø', '–ø–æ–ª–Ω—ã–π', '–ø–æ–¥—Ä–æ–±–Ω—ã–π', '—ç–∫—Å–ø–µ—Ä—Ç', '–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π']
        if any(word in title_lower for word in emotional_words):
            score += 0.2
        
        return min(1.0, score)
    
    def _analyze_content_seo(self, content: str, keywords: List[str]) -> float:
        """–ê–Ω–∞–ª–∏–∑ SEO –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        score = 0.0
        content_lower = content.lower()
        
        # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keyword_density = self._calculate_keyword_density(content, keywords)
        if 0.01 <= keyword_density <= 0.03:
            score += 0.4
        elif keyword_density > 0.03:
            score += 0.2
        
        # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        word_count = len(content.split())
        if word_count >= 300:
            score += 0.3
        if word_count >= 1000:
            score += 0.2
        
        # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
        if len(set(content.split())) / word_count > 0.7:
            score += 0.1
        
        return min(1.0, score)
    
    def _analyze_structure_seo(self, content: str) -> float:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è SEO"""
        score = 0.0
        
        # –ù–∞–ª–∏—á–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        h1_count = len(re.findall(r'^#\s+', content, re.MULTILINE))
        h2_count = len(re.findall(r'^##\s+', content, re.MULTILINE))
        h3_count = len(re.findall(r'^###\s+', content, re.MULTILINE))
        
        if h1_count == 1:
            score += 0.3
        if h2_count >= 2:
            score += 0.3
        if h3_count >= 1:
            score += 0.2
        
        # –ù–∞–ª–∏—á–∏–µ —Å–ø–∏—Å–∫–æ–≤
        if re.search(r'^\s*[-*+]\s+', content, re.MULTILINE):
            score += 0.1
        
        # –ù–∞–ª–∏—á–∏–µ –≤—ã–¥–µ–ª–µ–Ω–∏–π
        if re.search(r'\*\*.*?\*\*', content):
            score += 0.1
        
        return min(1.0, score)
    
    def _calculate_keyword_density(self, content: str, keywords: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        if not keywords:
            return 0.0
        
        content_lower = content.lower()
        total_words = len(content.split())
        
        keyword_count = 0
        for keyword in keywords:
            keyword_count += content_lower.count(keyword.lower())
        
        return keyword_count / total_words if total_words > 0 else 0.0
    
    def _analyze_meta_elements(self, content: str, title: str) -> float:
        """–ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞-—ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        score = 0.0
        
        # Meta description (–∏–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
        sentences = content.split('.')
        meta_description = sentences[0][:self.optimal_description_length]
        
        if 120 <= len(meta_description) <= self.optimal_description_length:
            score += 0.5
        
        # Alt-—Ç–µ–∫—Å—Ç –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–∑–∞–≥–ª—É—à–∫–∞)
        if '![' in content:
            score += 0.3
        
        # Schema markup (–∑–∞–≥–ª—É—à–∫–∞)
        score += 0.2
        
        return min(1.0, score)
    
    def _analyze_internal_links(self, content: str) -> float:
        """–ê–Ω–∞–ª–∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫"""
        score = 0.0
        
        # –ü–æ–¥—Å—á–µ—Ç —Å—Å—ã–ª–æ–∫
        link_count = len(re.findall(r'\[.*?\]\(.*?\)', content))
        
        if link_count >= 2:
            score += 0.5
        elif link_count >= 1:
            score += 0.3
        
        # –ö–∞—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ (–∑–∞–≥–ª—É—à–∫–∞)
        score += 0.5
        
        return min(1.0, score)
    
    def generate_meta_description(self, content: str, keywords: List[str]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏—è"""
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º
        sentences = content.split('.')
        first_sentence = sentences[0].strip()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        if keywords and keywords[0].lower() not in first_sentence.lower():
            first_sentence = f"{keywords[0]}: {first_sentence}"
        
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
        if len(first_sentence) > self.optimal_description_length:
            first_sentence = first_sentence[:self.optimal_description_length-3] + "..."
        
        return first_sentence
    
    def suggest_internal_links(self, content: str, topic: str) -> List[Dict[str, str]]:
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫"""
        # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø–æ—Å—Ç—ã –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        relevant_posts = Post.query.filter(
            Post.title.contains(topic) | Post.content.contains(topic)
        ).limit(5).all()
        
        links = []
        for post in relevant_posts:
            links.append({
                'title': post.title,
                'url': f'/post/{post.id}',
                'anchor_text': post.title[:50] + "..." if len(post.title) > 50 else post.title
            })
        
        return links

class AdvancedContentGenerator:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self):
        self.ai_integration = ModernAIIntegration()
        self.personalization = ContentPersonalization()
        self.seo_optimizer = SEOOptimizer()
        self.content_templates = self._load_content_templates()
        self.fact_checker = self._init_fact_checker()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        self.generation_stats = {
            'total_generated': 0,
            'by_content_type': defaultdict(int),
            'by_tone': defaultdict(int),
            'by_audience': defaultdict(int),
            'avg_quality_score': 0.0,
            'avg_seo_score': 0.0
        }
    
    def _load_content_templates(self) -> Dict[ContentType, Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —à–∞–±–ª–æ–Ω–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        return {
            ContentType.HOW_TO_GUIDE: {
                'structure': ['introduction', 'prerequisites', 'step_by_step', 'tips', 'conclusion'],
                'tone_keywords': ['–ø–æ—à–∞–≥–æ–≤–æ', '–ø–æ–¥—Ä–æ–±–Ω–æ', '–ø–æ–Ω—è—Ç–Ω–æ', '–ø—Ä–∞–∫—Ç–∏—á–Ω–æ'],
                'min_words': 800,
                'max_words': 2000
            },
            ContentType.COMPARISON_REVIEW: {
                'structure': ['introduction', 'criteria', 'comparison', 'pros_cons', 'recommendation'],
                'tone_keywords': ['–æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ', '—Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ', '–∞–Ω–∞–ª–∏—Ç–∏—á–Ω–æ'],
                'min_words': 1000,
                'max_words': 2500
            },
            ContentType.ANALYTICAL_ARTICLE: {
                'structure': ['introduction', 'data_analysis', 'trends', 'implications', 'conclusion'],
                'tone_keywords': ['–∞–Ω–∞–ª–∏—Ç–∏—á–Ω–æ', '–æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ', '–Ω–∞—É—á–Ω–æ'],
                'min_words': 1200,
                'max_words': 3000
            },
            ContentType.LISTICLE: {
                'structure': ['introduction', 'list_items', 'conclusion'],
                'tone_keywords': ['–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ', '–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å–Ω–æ', '—É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ'],
                'min_words': 600,
                'max_words': 1500
            }
        }
    
    def _init_fact_checker(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤"""
        return {
            'wikipedia_api': 'https://ru.wikipedia.org/api/rest_v1/page/summary/',
            'news_api': 'https://newsapi.org/v2/everything',
            'cache': {}
        }
    
    async def generate_content(self, request: ContentRequest) -> GeneratedContent:
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        start_time = time.time()
        
        try:
            # –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞
            if request.personalized and request.user_preferences:
                request = self.personalization.personalize_content_request(
                    request, request.user_preferences
                )
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title = await self._generate_title(request)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content = await self._generate_main_content(request, title)
            
            # –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if request.personalized and request.user_preferences:
                content = self.personalization.adapt_content_for_user(
                    content, request.user_preferences
                )
            
            # SEO-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
            seo_analysis = self.seo_optimizer.optimize_for_seo(content, title, request.keywords)
            meta_description = self.seo_optimizer.generate_meta_description(content, request.keywords)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            excerpt = self._generate_excerpt(content)
            tags = self._generate_tags(request.topic, request.keywords)
            internal_links = self.seo_optimizer.suggest_internal_links(content, request.topic)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤
            images_suggestions = await self._generate_image_suggestions(request.topic, content)
            social_posts = self._generate_social_media_posts(title, excerpt)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            word_count = len(content.split())
            reading_time = max(1, word_count // 200)
            readability_score = flesch_reading_ease(content)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result = GeneratedContent(
                title=title,
                content=content,
                excerpt=excerpt,
                meta_description=meta_description,
                tags=tags,
                category=request.topic,
                content_type=request.content_type,
                tone=request.tone,
                target_audience=request.target_audience,
                word_count=word_count,
                reading_time=reading_time,
                seo_score=seo_analysis['overall_seo_score'],
                readability_score=readability_score,
                engagement_score=self._calculate_engagement_score(content),
                quality_score=self._calculate_quality_score(content, seo_analysis),
                images_suggestions=images_suggestions,
                internal_links=internal_links,
                external_links=[],  # –ó–∞–≥–ª—É—à–∫–∞
                call_to_action=self._generate_call_to_action(request),
                social_media_posts=social_posts,
                generated_at=datetime.now(),
                processing_time=time.time() - start_time
            )
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self._update_generation_stats(result)
            
            # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            track_ai_content_generation(
                content, title, result.processing_time,
                result.processing_time * 0.4,  # generation_time
                result.processing_time * 0.3,  # validation_time
                result.processing_time * 0.3,  # correction_time
                True, request.topic
            )
            
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
            monitoring_system.error_tracker.record_error(e, {'function': 'generate_content'})
            raise
    
    async def _generate_title(self, request: ContentRequest) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        title_variations = self.ai_integration.generate_title_variations(
            request.topic, request.content_type
        )
        
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ SEO –∏ –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        best_title = title_variations[0]
        best_score = 0.0
        
        for title in title_variations[:5]:
            seo_score = self.seo_optimizer._analyze_title_seo(title, request.keywords)
            attractiveness_score = self._calculate_title_attractiveness(title)
            total_score = seo_score * 0.6 + attractiveness_score * 0.4
            
            if total_score > best_score:
                best_score = total_score
                best_title = title
        
        return best_title
    
    async def _generate_main_content(self, request: ContentRequest, title: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        template = self.content_templates.get(request.content_type, {})
        structure = template.get('structure', ['introduction', 'main_content', 'conclusion'])
        
        content_sections = []
        
        for section in structure:
            section_content = await self._generate_section_content(
                section, request, title
            )
            content_sections.append(section_content)
        
        return '\n\n'.join(content_sections)
    
    async def _generate_section_content(self, section: str, request: ContentRequest, 
                                      title: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å–µ–∫—Ü–∏–∏"""
        prompts = {
            'introduction': f"–ù–∞–ø–∏—à–∏ –≤–≤–µ–¥–µ–Ω–∏–µ –¥–ª—è —Å—Ç–∞—Ç—å–∏ '{title}' –Ω–∞ —Ç–µ–º—É '{request.topic}'. –¢–æ–Ω: {request.tone.value}. –ê—É–¥–∏—Ç–æ—Ä–∏—è: {request.target_audience.value}. –í–∫–ª—é—á–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(request.keywords[:3])}.",
            'main_content': f"–ù–∞–ø–∏—à–∏ –æ—Å–Ω–æ–≤–Ω—É—é —á–∞—Å—Ç—å —Å—Ç–∞—Ç—å–∏ –æ '{request.topic}'. –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {request.content_type.value}. –í–∫–ª—é—á–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.",
            'conclusion': f"–ù–∞–ø–∏—à–∏ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è —Å—Ç–∞—Ç—å–∏ –æ '{request.topic}'. –î–æ–±–∞–≤—å –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é –∏ –ø–æ–¥–≤–µ–¥–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤.",
            'step_by_step': f"–°–æ–∑–¥–∞–π –ø–æ—à–∞–≥–æ–≤–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ '{request.topic}'. –°–¥–µ–ª–∞–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–Ω—è—Ç–Ω—ã–º–∏ –¥–ª—è {request.target_audience.value}.",
            'comparison': f"–°–æ–∑–¥–∞–π —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ —Ç–µ–º–µ '{request.topic}'. –û–±—ä–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏ —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏ –ø–æ–¥—Ö–æ–¥—ã.",
            'tips': f"–î–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã –ø–æ —Ç–µ–º–µ '{request.topic}'. –°–¥–µ–ª–∞–π —Å–æ–≤–µ—Ç—ã –ø–æ–ª–µ–∑–Ω—ã–º–∏ –∏ –ø—Ä–∏–º–µ–Ω–∏–º—ã–º–∏."
        }
        
        prompt = prompts.get(section, f"–ù–∞–ø–∏—à–∏ —Ä–∞–∑–¥–µ–ª '{section}' –¥–ª—è —Å—Ç–∞—Ç—å–∏ –æ '{request.topic}'.")
        
        try:
            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPT-4
            content = await self.ai_integration.generate_with_gpt4(prompt, max_tokens=800)
            return content
        except:
            # Fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
            return self._generate_fallback_content(section, request)
    
    def _generate_fallback_content(self, section: str, request: ContentRequest) -> str:
        """Fallback –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        fallback_templates = {
            'introduction': f"–í —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ —Ç–µ–º–∞ {request.topic} —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤—Å–µ –±–æ–ª–µ–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π. –í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –º—ã –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –≤—Å–µ –∞—Å–ø–µ–∫—Ç—ã —ç—Ç–æ–π –≤–∞–∂–Ω–æ–π —Ç–µ–º—ã.",
            'main_content': f"–û—Å–Ω–æ–≤–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã {request.topic} –≤–∫–ª—é—á–∞—é—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–∞–∂–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤. –î–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä–µ–º –∏—Ö –ø–æ–¥—Ä–æ–±–Ω–æ.",
            'conclusion': f"–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, {request.topic} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤–∞–∂–Ω—É—é –æ–±–ª–∞—Å—Ç—å –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –∏ —Ä–∞–∑–≤–∏—Ç–∏—è. –ü–æ–Ω–∏–º–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –ø–æ–º–æ–∂–µ—Ç –≤ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏."
        }
        
        return fallback_templates.get(section, f"–†–∞–∑–¥–µ–ª –æ {request.topic}.")
    
    def _generate_excerpt(self, content: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è"""
        sentences = content.split('.')
        first_sentence = sentences[0].strip()
        
        if len(first_sentence) > 150:
            excerpt = first_sentence[:147] + "..."
        else:
            excerpt = first_sentence + "."
        
        return excerpt
    
    def _generate_tags(self, topic: str, keywords: List[str]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–≥–æ–≤"""
        base_tags = [topic] + keywords[:3]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ–≥–∏
        additional_tags = [
            '–ø–æ–ª–µ–∑–Ω–æ', '–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ', '–ø—Ä–∞–∫—Ç–∏—á–Ω–æ', '—Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ',
            '–∞–∫—Ç—É–∞–ª—å–Ω–æ', '–ø–æ–ø—É–ª—è—Ä–Ω–æ', '—Ç—Ä–µ–Ω–¥', '–Ω–æ–≤–∏–Ω–∫–∞'
        ]
        
        selected_additional = random.sample(additional_tags, 3)
        return base_tags + selected_additional
    
    async def _generate_image_suggestions(self, topic: str, content: str) -> List[Dict[str, str]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        suggestions = []
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        keywords = re.findall(r'\b\w{4,}\b', content.lower())
        keyword_counts = Counter(keywords)
        top_keywords = [word for word, count in keyword_counts.most_common(5)]
        
        for keyword in top_keywords[:3]:
            suggestions.append({
                'keyword': keyword,
                'description': f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, —Å–≤—è–∑–∞–Ω–Ω–æ–µ —Å {keyword}",
                'alt_text': f"–ò–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—è {keyword}",
                'suggested_source': 'unsplash'
            })
        
        return suggestions
    
    def _generate_social_media_posts(self, title: str, excerpt: str) -> Dict[str, str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π"""
        return {
            'twitter': f"{title[:100]}{'...' if len(title) > 100 else ''}",
            'facebook': f"{title}\n\n{excerpt[:200]}{'...' if len(excerpt) > 200 else ''}",
            'linkedin': f"–ù–æ–≤–∞—è —Å—Ç–∞—Ç—å—è: {title}\n\n{excerpt[:300]}{'...' if len(excerpt) > 300 else ''}",
            'telegram': f"üìñ {title}\n\n{excerpt}"
        }
    
    def _generate_call_to_action(self, request: ContentRequest) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑—ã–≤–∞ –∫ –¥–µ–π—Å—Ç–≤–∏—é"""
        ctas = {
            ContentType.HOW_TO_GUIDE: "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —ç—Ç–∏ —Å–æ–≤–µ—Ç—ã –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –∏ –ø–æ–¥–µ–ª–∏—Ç–µ—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏!",
            ContentType.COMPARISON_REVIEW: "–ö–∞–∫–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –≤—ã –±—ã –≤—ã–±—Ä–∞–ª–∏? –ü–æ–¥–µ–ª–∏—Ç–µ—Å—å —Å–≤–æ–∏–º –º–Ω–µ–Ω–∏–µ–º –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö!",
            ContentType.ANALYTICAL_ARTICLE: "–ß—Ç–æ –≤—ã –¥—É–º–∞–µ—Ç–µ –æ–± —ç—Ç–∏—Ö —Ç–µ–Ω–¥–µ–Ω—Ü–∏—è—Ö? –û–±—Å—É–¥–∏–º –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö!",
            ContentType.LISTICLE: "–ö–∞–∫–∏–µ –ø—É–Ω–∫—Ç—ã –≤–∞—Å –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–ª–∏? –†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö!"
        }
        
        return ctas.get(request.content_type, "–ü–æ–¥–µ–ª–∏—Ç–µ—Å—å —Å–≤–æ–∏–º –º–Ω–µ–Ω–∏–µ–º –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö!")
    
    def _calculate_title_attractiveness(self, title: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        score = 0.0
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        emotional_words = ['–ª—É—á—à–∏–π', '—Ç–æ–ø', '—Å–µ–∫—Ä–µ—Ç', '–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π', '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π', '—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π']
        if any(word in title.lower() for word in emotional_words):
            score += 0.3
        
        # –ß–∏—Å–ª–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
        if re.search(r'\d+', title):
            score += 0.2
        
        # –í–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        question_words = ['–∫–∞–∫', '—á—Ç–æ', '–ø–æ—á–µ–º—É', '–∫–æ–≥–¥–∞', '–≥–¥–µ']
        if any(word in title.lower() for word in question_words):
            score += 0.2
        
        # –î–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        if 30 <= len(title) <= 60:
            score += 0.3
        
        return min(1.0, score)
    
    def _calculate_engagement_score(self, content: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏"""
        score = 0.0
        
        # –ù–∞–ª–∏—á–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤
        question_count = content.count('?')
        if question_count > 0:
            score += min(0.3, question_count * 0.1)
        
        # –ù–∞–ª–∏—á–∏–µ —Å–ø–∏—Å–∫–æ–≤
        if re.search(r'^\s*[-*+]\s+', content, re.MULTILINE):
            score += 0.2
        
        # –ù–∞–ª–∏—á–∏–µ –≤—ã–¥–µ–ª–µ–Ω–∏–π
        if re.search(r'\*\*.*?\*\*', content):
            score += 0.2
        
        # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è)
        word_count = len(content.split())
        if 800 <= word_count <= 2000:
            score += 0.3
        
        return min(1.0, score)
    
    def _calculate_quality_score(self, content: str, seo_analysis: Dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
        readability_score = flesch_reading_ease(content) / 100
        seo_score = seo_analysis['overall_seo_score']
        engagement_score = self._calculate_engagement_score(content)
        
        return (readability_score * 0.4 + seo_score * 0.3 + engagement_score * 0.3)
    
    def _update_generation_stats(self, result: GeneratedContent):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        self.generation_stats['total_generated'] += 1
        self.generation_stats['by_content_type'][result.content_type.value] += 1
        self.generation_stats['by_tone'][result.tone.value] += 1
        self.generation_stats['by_audience'][result.target_audience.value] += 1
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏
        total = self.generation_stats['total_generated']
        current_avg_quality = self.generation_stats['avg_quality_score']
        current_avg_seo = self.generation_stats['avg_seo_score']
        
        self.generation_stats['avg_quality_score'] = (
            (current_avg_quality * (total - 1) + result.quality_score) / total
        )
        self.generation_stats['avg_seo_score'] = (
            (current_avg_seo * (total - 1) + result.seo_score) / total
        )
    
    def get_generation_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        return dict(self.generation_stats)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
advanced_content_generator = AdvancedContentGenerator()

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def generate_advanced_content(topic: str, content_type: str = "how_to_guide",
                                  tone: str = "conversational", 
                                  target_audience: str = "general_public",
                                  keywords: List[str] = None,
                                  personalized: bool = False,
                                  user_preferences: Dict[str, Any] = None) -> GeneratedContent:
    """–£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    request = ContentRequest(
        topic=topic,
        content_type=ContentType(content_type),
        tone=ContentTone(tone),
        target_audience=TargetAudience(target_audience),
        length="medium",
        keywords=keywords or [],
        exclude_keywords=[],
        personalized=personalized,
        user_preferences=user_preferences
    )
    
    return await advanced_content_generator.generate_content(request)

def get_advanced_generator_stats() -> Dict[str, Any]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞"""
    return advanced_content_generator.get_generation_statistics()